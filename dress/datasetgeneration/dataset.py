import glob
import os
import re
from typing import List, Union

import pandas as pd

import numpy as np
from dress.datasetgeneration.archive import Archive


class Dataset(object):
    def __init__(
        self,
        dataset: Union[str, list, pd.DataFrame],
        group: Union[str, None] = "1",
    ) -> None:
        """Representation of an evolved dataset

        Args:
            dataset (Union[str, list, pd.DataFrame]): If provided as a DataFrame and 'id' and 'group' columns exist,
        'id' will be re-incremented, and 'group' will be overridden by the 'group' argument.
            group (str, optional): Group of the dataset(s) (e.g. 'positive', 'control'). Defaults to 1.
        """

        if group:
            assert isinstance(group, str), "Group must be a string"

        self.group = group
        if isinstance(dataset, pd.DataFrame):
            self.data: pd.DataFrame = self._load_from_df(dataset)

        else:
            self.data: pd.DataFrame = self._load(dataset)
        self.dataset_size = len(self.data)

        # Attributes
        self._dt_id = self.data.iloc[0].Seq_id
        self._wt_seq_size = len(self.data.iloc[0].Sequence)
        self._wt_seq = self.data.iloc[0].Sequence
        self._wt_score = self.data.iloc[0].Score
        self._wt_ss = self.data.iloc[0].Splice_site_positions
        #self._metrics = Archive(dataset=self.data.iloc[1:]).metrics
        #self._quality = Archive(dataset=self.data.iloc[1:]).quality

    def __len__(self):
        return self.dataset_size

    def __str__(self) -> str:
        return "Dataset with {} sequences".format(self.__len__())

    @property
    def id(self):
        return self._dt_id

    @property
    def wt_sequence(self):
        return self._wt_seq

    @property
    def wt_sequence_size(self):
        return self._wt_seq_size

    @property
    def wt_score(self):
        return self._wt_score

    @property
    def wt_splice_sites(self):
        return self._wt_ss

    @property
    def wt_splice_sites_list(self):
        _l = [int(x) if x != "<NA>" else pd.NA for x in self._wt_ss.split(";")]
        return [_l[0:2], _l[2:4], _l[4:6]]

    @property
    def ngroups(self):
        return 1

    @property
    def metrics(self):
        return self._metrics

    @property
    def quality(self):
        return self._quality

    def _load_from_df(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Load dataset(s) from a dataframe.

        Args:
            df (pd.DataFrame): Dataframe with the dataset (e.g, generated from a dress filter operation)

        Returns:
            pd.DataFrame: Loaded dataset
        """
        dataset = df.copy()
        expected_cols = [
            "Run_id",
            "Seed",
            "Seq_id",
            "Phenotype",
            "Sequence",
            "Splice_site_positions",
            "Score",
            "Delta_score",
        ]
        assert all(
            c in dataset.columns for c in expected_cols
        ), "Dataset loaded from a dataframe must have the following columns: {}".format(
            "\n".join(expected_cols)
        )
        dataset = dataset[expected_cols]
        if self.group:
            dataset["group"] = self.group

        dataset = _remove_duplicates_across_seeds(dataset).reset_index(drop=True)
        dataset = _increment_seq_id(dataset)
        return _create_integer_id(dataset)

    def _load(self, dataset=Union[str, list, tuple]):
        """
        Load evolved dataset(s) into a single dataframe

        Args:
            dataset (Union[str, list, tuple]): Files generated by one (or multiple) evolutionary processes

        Returns:
            pd.DataFrame: Loaded dataset(s)
        """

        if isinstance(dataset, str):
            if os.path.isdir(dataset):
                files = glob.glob(f"{dataset}/*dataset.csv.gz")
                if len(files) == 0:
                    raise ValueError(f"No valid dataset files found in {dataset}")
                elif len(files) == 1:
                    df = pd.read_csv(files[0])
                else:
                    df = _read_multiple_files(files)

            elif os.path.isfile(dataset):
                df = pd.read_csv(dataset)

            # If multiple datasets within a single file, change a column to identify them
            if df.groupby("Seq_id").ngroups > 1:
                df["dataset"] = pd.factorize(df.Seq_id)[0].astype(np.uint16)

        # List of files
        elif isinstance(dataset, Union[list]):

            for f in dataset:
                assert os.path.isfile(f), f"File {f} not found"

            if len(dataset) > 1:
                df = _read_multiple_files(dataset)

            else:
                df = pd.read_csv(dataset[0])
                if df.groupby("Seq_id").ngroups > 1:
                    df["dataset"] = pd.factorize(df.Seq_id)[0].astype(np.uint16)

        if self.group:
            df["group"] = self.group

        df.loc[df.Phenotype == "wt", "Seq_id"] += "_wt"
        df = _remove_duplicates_across_seeds(df).reset_index(drop=True)
        df = _increment_seq_id(df)
        return _create_integer_id(df)


class PairedDataset(object):
    def __init__(self, dataset1: Dataset, dataset2: Dataset) -> None:
        """

        Args:
            dataset1 (Dataset): Dataset 1
            dataset2 (Dataset): Dataset 2
        """

        assert list(dataset1.data) == list(
            dataset2.data
        ), "Both datasets must have the same columns"

        assert dataset1.group != dataset2.group, "Datasets must have different groups"

        self.dataset1 = dataset1
        self.dataset2 = dataset2

        data = pd.concat([self.dataset1.data, self.dataset2.data]).reset_index(
            drop=True
        )

        self.data = _create_integer_id(data)
        self.dataset1.data = None
        self.dataset2.data = None

    def __len__(self):
        return len(self.data)

    def __str__(self) -> str:
        n_g1 = len(self.data[self.data.group == self.dataset1.group])
        n_g2 = len(self.data[self.data.group == self.dataset2.group])
        return f"Paired Dataset with {self.__len__()} sequences (group {self.dataset1.group}:{n_g1}, group {self.dataset2.group}:{n_g2})"

    @property
    def ngroups(self):
        return 2


def structure_dataset(
    generated_datasets: List[tuple], **kwargs
) -> Union[Dataset, PairedDataset]:
    """Configs the datasets to be evaluated

    Args:
        generated_datasets (List[tuple]): Generated dataset(s) to be evaluated

    Returns:
        Dataset: Configured dataset(s)
    """
    _g1 = kwargs["groups"][0] if kwargs["groups"] else "1"

    def _create_single_dataset(data, group):
        return Dataset(data[0] if len(data) == 1 else list(data), group=group)

    dataset1 = _create_single_dataset(generated_datasets[0], _g1)

    if generated_datasets[1]:
        _g2 = kwargs["groups"][1] if kwargs["groups"] else "2"
        dataset2 = _create_single_dataset(
            generated_datasets[1],
            _g2,
        )
        return PairedDataset(dataset1, dataset2)

    return dataset1


def _read_multiple_files(files: list) -> pd.DataFrame:
    """
    Read multiple files into a single dataframe

    Args:
        files (list): List of dataset files

    Returns:
        pd.DataFrame: Single dataframe with all datasets
    """
    dfs = []
    for i, d in enumerate(files):
        aux = pd.read_csv(d)
        n_seqs = aux.groupby("Seq_id").ngroups
        assert n_seqs == 1, (
            "Because multiple files were provided in '--datasets', it is expected that each file represents "
            f"a single dataset (evolved from a single sequence). However, {n_seqs} were found in {d} file"
        )

        if "Seed" not in aux.columns:
            aux["Seed"] = i

        dfs.append(aux)
    return pd.concat(dfs)


def _remove_duplicates_across_seeds(df: pd.DataFrame) -> pd.DataFrame:
    """Remove duplicated sequences generated across different evolutionary processes

    Args:
        df (pd.DataFrame): Dataset with possible duplicated sequences

    Returns:
        pd.DataFrame: Dataset without duplicated sequences
    """
    if df.Seed.nunique() == 1:
        return df

    return (
        df.sort_values(by="Seed")
        .drop_duplicates(subset=["Sequence", "Phenotype"])
        .drop_duplicates("Sequence", keep="first")
    )


def _increment_seq_id(df: pd.DataFrame) -> pd.DataFrame:
    """Create unique sequence IDs for a given dataset

    Args:
        df (pd.DataFrame): Dataset

    Returns:
        pd.DataFrame: Dataset with unique sequence IDs
    """
    assert df.iloc[0].Phenotype == "wt", "First sequence must be the original one"

    df_count = df.groupby("Seq_id").size().reset_index(name="counts")

    df = df.merge(df_count, on="Seq_id", how="left")
    df["_counter"] = (
        df.groupby("Seq_id").cumcount().astype(str).where(df["counts"] > 1, "")
    )

    df["Seq_id"] = (
        df.Seq_id
        + "_g"
        + df.group
        + df._counter.apply(lambda x: "_" + x if x != "" else "")
    )
    df = df.drop(columns=["counts", "_counter"])

    return df


def _create_integer_id(df: pd.DataFrame) -> pd.DataFrame:
    """Create unique integer IDs for a given dataset

    Args:
        df (pd.DataFrame): Dataset

    Returns:
        pd.DataFrame: Dataset with an unique integer ID
    """
    df = df.reset_index(drop=True)
    df["id"] = range(len(df))
    return df
