import re
from typing import List, Union

import pandas as pd

import numpy as np
from dress.datasetgeneration.archive import Archive


class Dataset(object):
    def __init__(
        self,
        dataset: Union[str, list, pd.DataFrame],
        original_seq: str = None,
        group: Union[str, None] = None,
    ) -> None:
        """Representation of an evolved dataset

        Args:
            dataset (Union[str, list, pd.DataFrame]): If provided as a DataFrame and 'id' and 'group' columns exist,
        'id' will be re-incremented, and 'group' will be overridden by the 'group' argument.
            original_seq (str, optional): Info file of the original sequence
            group (str, optional): Group of the dataset(s) (e.g. 'positive', 'control')
        """

        if group:
            assert isinstance(group, str), "Group must be a string"

        self.group = group
        if isinstance(dataset, pd.DataFrame):
            self.data: pd.DataFrame = self._load_from_df(dataset)
            self.dataset_size = len(self.data)

        else:
            self.data: pd.DataFrame = self._load(dataset)
            self.dataset_size = len(self.data)

        if original_seq:
            self.info = pd.read_csv(original_seq).drop_duplicates()
            assert len(self.info) == 1
        else:
            self.info = _synthetic2original(self.data)

    def __len__(self):
        try:
            return len(self.data)
        except TypeError:
            return self.dataset_size

    def __str__(self) -> str:
        return "Dataset with {} sequences".format(self.__len__())

    @property
    def id(self):
        return self.info.Seq_id.unique()[0]

    @property
    def sequence(self):
        return self.info.Sequence.unique()[0]

    @property
    def sequence_size(self):
        return len(self.info.Sequence.unique()[0])

    @property
    def score(self):
        return self.info.Score.unique()[0]

    @property
    def splice_sites(self):
        return self.info.Splice_site_positions.unique()[0]

    @property
    def splice_sites_list(self):
        _l = [int(x) if x != "<NA>" else pd.NA for x in self.splice_sites.split(";")]
        return [_l[0:2], _l[2:4], _l[4:6]]

    @property
    def ngroups(self):
        return 1

    @property
    def metrics(self):
        return Archive(dataset=self.data).metrics

    @property
    def quality(self):
        return Archive(dataset=self.data).quality
 
    def _load_from_df(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Load dataset(s) from a dataframe.

        Args:
            df (pd.DataFrame): Dataframe with the dataset (e.g, generated from a dress filter operation)

        Returns:
            pd.DataFrame: Loaded dataset
        """
        dataset = df.copy()
        expected_cols = [
            "Run_id",
            "Seed",
            "Seq_id",
            "Phenotype",
            "Sequence",
            "Splice_site_positions",
            "Score",
            "Delta_score",
        ]
        assert all(
            c in dataset.columns for c in expected_cols
        ), "Dataset loaded from a dataframe must have the following columns: {}".format(
            "\n".join(expected_cols)
        )
        dataset = dataset[expected_cols]
        if self.group:
            dataset["group"] = self.group

        dataset = self._remove_duplicates_across_seeds(dataset).reset_index(drop=True)
        dataset = self._increment_seq_id(dataset)
        return self._create_integer_id(dataset)

    def _load(self, dataset=Union[str, list]):
        """
        Load evolved dataset(s) into a single dataframe

        Args:
            dataset (Union[str, list]): Files generated by one (or multiple) evolutionary processes

        Returns:
            pd.DataFrame: Loaded dataset(s)
        """

        # If dataset(s) is represented in a single file
        if isinstance(dataset, str):
            df = pd.read_csv(dataset)

            # If multiple datasets within a single file, change a column to identify them
            if df.groupby("Seq_id").ngroups > 1:
                df["dataset"] = pd.factorize(df.Seq_id)[0].astype(np.uint16)

        elif isinstance(dataset[0], str):
            dfs = []

            if len(dataset) > 1:
                for i, d in enumerate(dataset):
                    aux = pd.read_csv(d)
                    n_seqs = aux.groupby("Seq_id").ngroups
                    assert n_seqs == 1, (
                        "Because multiple files were provided in '--datasets', it is expected that each file represents "
                        f"a single dataset (evolved from a single sequence). However, {n_seqs} were found in {d} file"
                    )

                    if "Seed" not in aux.columns:
                        aux["Seed"] = i
                    dfs.append(aux)

            else:
                df = pd.read_csv(dataset[0])
                if df.groupby("Seq_id").ngroups > 1:
                    df["dataset"] = pd.factorize(df.Seq_id)[0].astype(np.uint16)
                dfs.append(df)

            df = pd.concat(dfs)

        elif isinstance(dataset[0], tuple):
            df = pd.DataFrame(
                dataset,
                columns=[
                    "Seq_id",
                    "Phenotype",
                    "fitness",
                    "Sequence",
                    "Splice_site_positions",
                ],
            )

        if self.group:
            df["group"] = self.group

        df = self._remove_duplicates_across_seeds(df).reset_index(drop=True)
        df = self._increment_seq_id(df)
        return self._create_integer_id(df)

    def _remove_duplicates_across_seeds(self, df: pd.DataFrame) -> pd.DataFrame:
        """Remove duplicated sequences generated across different evolutionary processes

        Args:
            df (pd.DataFrame): Dataset with possible duplicated sequences

        Returns:
            pd.DataFrame: Dataset without duplicated sequences
        """
        if df.Seed.nunique() == 1:
            return df

        return (
            df.sort_values(by="Seed")
            .drop_duplicates(subset=["Sequence", "Phenotype"])
            .drop_duplicates("Sequence", keep="first")
        )

    def _increment_seq_id(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create unique sequence IDs for a given dataset

        Args:
            df (pd.DataFrame): Dataset

        Returns:
            pd.DataFrame: Dataset with unique sequence IDs
        """

        df_count = df.groupby("Seq_id").size().reset_index(name="counts")
        df = df.merge(df_count, on="Seq_id", how="left")
        df["_counter"] = (
            df.groupby("Seq_id").cumcount().astype(str).where(df["counts"] > 1, "")
        )
        df["Seq_id"] = df["Seq_id"] + df["_counter"].apply(
            lambda x: "_" + x if x != "" else ""
        )
        df = df.drop(columns=["counts", "_counter"])

        return df

    def _create_integer_id(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create unique integer IDs for a given dataset

        Args:
            df (pd.DataFrame): Dataset

        Returns:
            pd.DataFrame: Dataset with unique sequence IDs
        """
        return df.reset_index().rename(columns={"index": "id"})


class PairedDataset(object):
    def __init__(self, dataset1: Dataset, dataset2: Dataset) -> None:
        """

        Args:
            dataset1 (Dataset): Dataset 1
            dataset2 (Dataset): Dataset 2
        """

        assert list(dataset1.data) == list(
            dataset2.data
        ), "Both datasets must have the same columns"

        assert dataset1.group != dataset2.group, "Datasets must have different groups"

        self.dataset1 = dataset1
        self.dataset2 = dataset2

        self.data = pd.concat([self.dataset1.data, self.dataset2.data]).reset_index(
            drop=True
        )
        self.data["id"] = range(len(self.data))

        # Clean memory
        self.dataset1.data = None
        self.dataset2.data = None

    def __len__(self):
        return len(self.data)

    def __str__(self) -> str:
        n_g1 = len(self.data[self.data.group == self.dataset1.group])
        n_g2 = len(self.data[self.data.group == self.dataset2.group])
        return "Paired Dataset with {} sequences (group {}:{}, group {}:{})".format(
            self.__len__(), self.dataset1.group, n_g1, self.dataset2.group, n_g2
        )

    @property
    def ngroups(self):
        return 2


def structure_dataset(
    generated_datasets: List[str], original_seqs: List[str] = None, **kwargs
) -> Union[Dataset, PairedDataset]:
    """Configs the datasets to be evaluated

    Args:
        generated_datasets (List[str]): Generated dataset(s) to be evaluated
        original_seqs (List[str]): Original sequence(s) used to generate the dataset(s)

    Returns:
        Dataset: Configured dataset(s)
    """
    _g1 = kwargs["groups"][0] if kwargs["groups"] else "1"
    
    def _create_single_dataset(data, original_seq, group):
        if original_seq:
            return Dataset(data, original_seq, group)
        else:
            return Dataset(data, group=group)

    dataset1 = _create_single_dataset(
        generated_datasets[0], original_seqs[0] if original_seqs else None, _g1
    )

    if generated_datasets[1]:
        _g2 = kwargs["groups"][1] if kwargs["groups"] else "2"
        dataset2 = _create_single_dataset(
            generated_datasets[1],
            original_seqs[1] if original_seqs else None,
            _g2,
        )
        return PairedDataset(dataset1, dataset2)

    return dataset1

@DeprecationWarning
def _synthetic2original(df: pd.DataFrame) -> pd.DataFrame:
    """Extracts information of the original sequence
    given a synthetic sequence generated out of it

    Args:
        df (pd.DataFrame): A synthetic dataset

    Returns:
        pd.DataFrame: A df with the original sequence info
    """
    # Ambiguous nucleotides to insert in the original
    # sequence when the true nucleotide is unknown (e.g,SNVs)
    ambiguous = {"ACG": "V", "ACT": "H", "AGT": "D", "CGT": "B"}

    def _get_original_seq(row: pd.Series):
        diffunits = row.Phenotype.split("|")
        edited_seq = row.Sequence

        _ss_idx = list(map(int, row.Splice_site_positions.split(";")))

        for diffunit in diffunits:
            if diffunit.startswith("SNV"):
                match = re.search(r"\[(\d+),([A-Za-z]+)\]", diffunit)
                if match:
                    pos = int(match.group(1))
                    nuc = ambiguous["ACGT".replace(match.group(2), "")]
                    edited_seq = edited_seq[:pos] + nuc + edited_seq[pos + 1 :]

                else:
                    raise ValueError(
                        f"Could not properly parse position in SNV {diffunit}."
                    )

            elif diffunit.startswith("RandomDeletion"):
                match = re.search(r"\[(\d+),(\d+)\]", diffunit)
                if match:
                    pos_start, pos_end = map(int, match.groups())
                    del_size = pos_end - pos_start + 1
                    edited_seq = (
                        edited_seq[:pos_start] + "N" * del_size + edited_seq[pos_start:]
                    )
                    _ss_idx = [ss + del_size if ss > pos_end else ss for ss in _ss_idx]
                else:
                    raise ValueError(
                        f"Could not properly parse position in RandomDeletion {diffunit}."
                    )

            elif diffunit.startswith("RandomInsertion"):
                match = re.search(r"\[(\d+),([A-Za-z]+)\]", diffunit)

                if match:
                    pos, nucs = int(match.group(1)), match.group(2)
                    edited_seq = edited_seq[:pos] + edited_seq[pos + len(nucs) :]
                    _ss_idx = [ss - len(nucs) if ss > pos else ss for ss in _ss_idx]

                else:
                    raise ValueError(
                        f"Could not properly parse position in RandomInsertion {diffunit}."
                    )

        return edited_seq, _ss_idx

    seq, ss_idx = df.head(1).apply(_get_original_seq, axis=1).iloc[0]

    return pd.DataFrame(
        {
            "Seq_id": [df["Seq_id"].iloc[0].rsplit("_", 1)[0]],
            "Sequence": [seq],
            "Splice_site_positions": [";".join(map(str, ss_idx))],
            "Score": [round(df.Score.iloc[0] - df.Delta_score.iloc[0], 4)],
        }
    )
